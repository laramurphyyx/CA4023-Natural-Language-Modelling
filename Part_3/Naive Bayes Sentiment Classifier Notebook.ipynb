{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Create mega-document for all negative reviews and another for all positive reviews by concatenating them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Concatenating Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_directory = '../Part_3/review_polarity/txt_sentoken/pos'\n",
    "positive_reviews_list = []\n",
    "\n",
    "for filename in os.listdir(pos_directory):\n",
    "    file_path = pos_directory + \"/\" + filename\n",
    "    file = open(file_path, \"r\")\n",
    "    review = []\n",
    "    for line in file.readlines():\n",
    "        review.append(line.rstrip())\n",
    "    positive_reviews_list.append(\" \".join(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_directory = '../Part_3/review_polarity/txt_sentoken/neg'\n",
    "negative_reviews_list = []\n",
    "\n",
    "for filename in os.listdir(neg_directory):\n",
    "    file_path = neg_directory + \"/\" + filename\n",
    "    file = open(file_path, \"r\")\n",
    "    review = []\n",
    "    for line in file.readlines():\n",
    "        review.append(line.rstrip())\n",
    "    negative_reviews_list.append(\" \".join(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('../Part_3/review_polarity/txt_sentoken/pos/cv000_29590.txt', \"r\")\n",
    "file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to lowercase and remove punctuation\n",
    "# email the bert stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baye's rule (for document d and class c) is P(c|d) = ( P(d|c)P(c) ) / P(d)\n",
    "\n",
    "This simplifies to P(c|d) = P(d|c)P(c) because we can drop the denominator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the most likely class, we find Cmap = argmax P(c|d)\n",
    "\n",
    "This can also be simplified to Cmap = argmax P(d|c) P(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(d|c) is called the likelihood\n",
    "\n",
    "P(c) is called the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a document has features x1, x2, ..., xn, then the likelihood is\n",
    "\n",
    "P(d|c) = P(x1, x2, ..., xn|c)P(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to assume that the features x1 to xn are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(x1, x2, ..., xn|c) = P(x1|c)P(x2|c)...P(xn|c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since P(c) is the probability of the class occuring, this will be 50% for both classes, positive and negative. This is because we have an equal amount of positive reviews and negative reviews in our test dataset, giving both equal probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_reviews_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of multiplying P(x1|c)P(x2|c)...P(xn|c), we will perform this calculation in the log space.\n",
    "\n",
    "This will mean that the result will no longer represent a probability, but it will still have the same properties (the higher the result, the more likely it is correct).\n",
    "\n",
    "We now have argmax[ log(P(c)) + sum of log(P(x|c) for all x) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(cj) = # of documents of class cj / total # of documents\n",
    "\n",
    "P(wj|ci) = # of times word wj appears in class ci / total # of words that appear in class ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a large dataset, however it is still possible that there will be 0 occurences for certain words. \n",
    "\n",
    "To combat these words achieving a 0 probability, we use LaPlace Smoothing.\n",
    "\n",
    "We now have P(xi|c) = ((# of times word wj appears in class ci) + 1) / ((total # of words that appear in class ci) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be worth considering a clause to deal with words that don't appear in any reviews (positive/negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could remove stop words, however this has not shown to be very beneficial and is not common practice when using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning Important Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning the test and training data\n",
    "negative_training_data = negative_reviews_list[:900]\n",
    "positive_training_data = positive_reviews_list[:900]\n",
    "negative_test_data = negative_reviews_list[900:]\n",
    "positive_test_data = positive_reviews_list[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_dictionary(list_of_reviews):\n",
    "    review_dictionary = {}\n",
    "    for review in list_of_reviews:\n",
    "        words = review.split()\n",
    "        for word in words:\n",
    "            if word in review_dictionary:\n",
    "                review_dictionary[word] += 1\n",
    "            else:\n",
    "                review_dictionary[word] = 1\n",
    "    return review_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_dictionary = create_frequency_dictionary(negative_training_data)\n",
    "positive_dictionary = create_frequency_dictionary(positive_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_probability = len(positive_training_data) / (len(positive_training_data)+len(negative_training_data))\n",
    "negative_probability = len(negative_training_data) / (len(positive_training_data)+len(negative_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_for_class(review, class_probability, training_data):\n",
    "    class_result = math.log(class_probability)\n",
    "    dictionary = create_frequency_dictionary(training_data)\n",
    "    for word in review.split():\n",
    "        if word in ['.', ',', ':', '\"', '&', '?', '-', '(', ')']:\n",
    "            continue\n",
    "        elif word not in dictionary:\n",
    "            class_result += math.log(1 / (sum(dictionary.values()) + 1))\n",
    "        else:\n",
    "            class_result += math.log((dictionary[word] + 1) / (sum(dictionary.values()) + 1))\n",
    "    return class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1846.3421536309943"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_for_class(positive_test_data[1], positive_probability, positive_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1841.1022259865354"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_for_class(positive_test_data[1], negative_probability, negative_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_class(review):\n",
    "    positive_class_result = math.log(positive_probability)\n",
    "    negative_class_result = math.log(negative_probability)\n",
    "    for word in review.split():\n",
    "        if word in ['.', ',', ':', '\"', '&', '?', '-', '(', ')']:\n",
    "            continue\n",
    "        else:\n",
    "            if word not in positive_dictionary and word not in negative dictionary:\n",
    "                continue\n",
    "            elif word not in positive_dictionary:\n",
    "                positive_class_result += math.log(\n",
    "                1 / (sum(positive_dictionary.values()) + 1))\n",
    "            elif word not in negative_dictionary:\n",
    "                negative_class_result += math.log(\n",
    "                1 / (sum(negative_dictionary.values()) + 1))\n",
    "            else:\n",
    "                positive_class_result += math.log(\n",
    "                    (positive_dictionary[word] + 1) / (sum(positive_dictionary.values()) + 1))\n",
    "                negative_class_result += math.log(\n",
    "                    (negative_dictionary[word] + 1) / (sum(negative_dictionary.values()) + 1))\n",
    "    if positive_class_result > negative_class_result:\n",
    "        return \"Positive\"\n",
    "    elif negative_class_result > positive_class_result:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Equal Probability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "total = 0\n",
    "\n",
    "for review in positive_test_data:\n",
    "    positive_result = result_for_class(review, positive_probability, positive_training_data)\n",
    "    negative_result = result_for_class(review, negative_probability, negative_training_data)\n",
    "    if positive_result > negative_result:\n",
    "        accuracy += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not have amazing performance on the positive reviews, let's see if it performs better in the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "total = 0\n",
    "\n",
    "for review in negative_test_data:\n",
    "    positive_result = result_for_class(review, positive_probability, positive_training_data)\n",
    "    negative_result = result_for_class(review, negative_probability, negative_training_data)\n",
    "    if positive_result < negative_result:\n",
    "        accuracy += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
